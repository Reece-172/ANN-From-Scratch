{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition as skd\n",
    "import sklearn.preprocessing as skp\n",
    "import pickle as pk\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):#normalize the data\n",
    "    return skp.normalize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "    data = normalize(data)\n",
    "    pca = skd.PCA(n_components=n)\n",
    "    pca.fit(data)\n",
    "    result = pca.transform(data)\n",
    "    pk.dump(pca, open('pca.pkl', 'wb'))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"inputs.txt\")\n",
    "labels = np.loadtxt(\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape #m is the number of data-points /samples, n is the number of features\n",
    "n =  50 #training with 50 features out of 2352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "data = data[randomize]\n",
    "labels = labels[randomize]\n",
    "\n",
    "data = pca(data, n)\n",
    "\n",
    "training_data = data.T\n",
    "Y_training = labels\n",
    "X_training = training_data[0:n]\n",
    "\n",
    "# validation_data = data[1200:1600].T\n",
    "# Y_valid = labels[1200:1600]\n",
    "# X_valid = validation_data[0:n]\n",
    "\n",
    "# testing_data = data[1600:].T\n",
    "# Y_testing = labels[1600:]\n",
    "# X_testing = testing_data[0:n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(): #generate a random set of weights and biases for the neural network between -1 and 1\n",
    "    w1= np.random.rand(50, n) - 1 # n is the number of features\n",
    "    b1 = np.random.rand(50, 1) - 1\n",
    "    w2 = np.random.rand(50, 50) - 1\n",
    "    b2 = np.random.rand(50, 1) - 1\n",
    "    w3 = np.random.rand(10, 50) - 1\n",
    "    b3 = np.random.rand(10, 1) - 1\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, b1, w2, b2, w3, b3): #forward propagation\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return A1, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels): #encode labels as one-hot vectors\n",
    "    labels = labels.astype(int)\n",
    "    encoded_labels = np.zeros((labels.size, 10))\n",
    "    for i in range(labels.size):\n",
    "        encoded_labels[i][labels[i]] = 1\n",
    "    return encoded_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(w1, w2, w3): #regularization (lambda = 0.99)\n",
    "    w1 = w1 * 0.78\n",
    "    w2 = w2 * 0.78\n",
    "    w3 = w3 * 0.78\n",
    "    return w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#w1, b1, w2, b2, w3, b3: weights and biases\n",
    "#X: training data\n",
    "#Y: training labels\n",
    "#A1, A2, A3: activation functions\n",
    "\n",
    "#Returns:\n",
    "#dw1, db1, dw2, db2, dw3, db3: deltas for weights and biases\n",
    "def backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3): #backpropagation\n",
    "    w1, w2, w3 = regularize(w1, w2, w3)\n",
    "    Y = one_hot_encode(Y)\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T) \n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(w3.T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid_prime(A2)\n",
    "    dW2 = np.dot(dZ2, A1.T) \n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(w2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid_prime(A1)\n",
    "    dW1 = np.dot(dZ1, X.T) \n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#dW1, db1, dW2, db2, dW3, db3: deltas for weights and biases\n",
    "\n",
    "#Returns:   \n",
    "#w1, b1, w2, b2, w3, b3: updated weights and biases\n",
    "def update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate): #update parameters\n",
    "    w1 = w1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    w3 = w3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A3): \n",
    "    return np.argmax(A3,0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    accuracy = np.sum(predictions == Y) / Y.size\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://towardsdatascience.com/simple-guide-to-hyperparameter-tuning-in-neural-networks-3fe03dad8594\n",
    "def gradient_descent(X, Y, epochs, learning_rate): #gradient descent -> learn weights and biases\n",
    "    w1, b1, w2, b2, w3, b3 = rand_params()\n",
    "    accuracy = []\n",
    "    for i in range(epochs):\n",
    "        A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3)\n",
    "        w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "        acc = accuracy_score(Y, get_predictions(A3))\n",
    "        accuracy.append(acc)\n",
    "        \n",
    "    return w1, b1, w2, b2, w3, b3, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  3000 | Training Accuracy:  99.8 %\n"
     ]
    }
   ],
   "source": [
    "epochs = 3000\n",
    "alpha = 0.01\n",
    "#learnt set of weights and biases (this is basically what we are submitting)\n",
    "w1, b1, w2, b2, w3, b3, accuracy = gradient_descent(X_training, Y_training, epochs, alpha)\n",
    "\n",
    "training_accuracy = accuracy_score(Y_training,get_predictions(forward_prop(X_training, w1, b1, w2, b2, w3, b3)[2]))*100\n",
    "print(\"Epochs: \",epochs,\"|\",\"Training Accuracy: \", training_accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "itr  = np.arange(0,epochs)\n",
    "plt.plot(itr, accuracy)\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing_accuracy = accuracy_score(Y_testing,get_predictions(forward_prop(X_testing, w1, b1, w2, b2, w3, b3)[2]))*100\n",
    "# print(\"Epochs: \",epochs,\"|\",\"Testing Accuracy: \", testing_accuracy,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w1 = np.loadtxt('w1.txt')\n",
    "# b1 = np.loadtxt('b1.txt')\n",
    "# w2 = np.loadtxt('w2.txt')\n",
    "# b2 = np.loadtxt('b2.txt')\n",
    "# w3 = np.loadtxt('w3.txt')\n",
    "# b3 = np.loadtxt('b3.txt')\n",
    "# b1 = b1.reshape(50,1)\n",
    "# b2 = b2.reshape(50,1)\n",
    "# b3 = b3.reshape(10,1)\n",
    "\n",
    "np.savetxt('w1.txt', w1)\n",
    "np.savetxt('b1.txt', b1)\n",
    "np.savetxt('w2.txt', w2)\n",
    "np.savetxt('b2.txt', b2)\n",
    "np.savetxt('w3.txt', w3)\n",
    "np.savetxt('b3.txt', b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "# import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cf_matrix = confusion_matrix(labels,get_predictions(forward_prop(data.T, w1, b1, w2, b2, w3, b3)[2]))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sns.heatmap(cf_matrix, annot=True, xlabel='Predicted', ylabel='True')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy_score(labels,get_predictions(forward_prop(data.T, w1, b1, w2, b2, w3, b3)[2]))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce3b179e982818025b1b8f06d5ea688e0dcbdafdcad235aa6481c10d2e91eb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
