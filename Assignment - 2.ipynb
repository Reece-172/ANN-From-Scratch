{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.preprocessing import KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8675\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt(\"inputs.txt\")\n",
    "labels = np.loadtxt(\"labels.txt\")\n",
    "\n",
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "data = data[randomize]\n",
    "labels = labels[randomize]\n",
    "\n",
    "training_data = data[:1200]\n",
    "Y_training = labels[:1200]\n",
    "\n",
    "validation_data = data[1200:1600]\n",
    "Y_validation = labels[1200:1600]\n",
    "\n",
    "testing_data = data[1600:]\n",
    "Y_testing = labels[1600:]\n",
    "\n",
    "# disc = KBinsDiscretizer(n_bins=10, encode='onehot', strategy='uniform')\n",
    "# disc.fit_transform(training_data)\n",
    "\n",
    "\n",
    "clf = MLPClassifier(activation='logistic', solver = 'adam', max_iter=3000).fit(training_data, Y_training)\n",
    "print(clf.score(testing_data, Y_testing))\n",
    "print(len(clf.coefs_))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data,m):#normalize the data\n",
    "    avgs=data.mean(axis=0) #calculates the means of each feature (each column)\n",
    "    averages=avgs.reshape((2352,1))\n",
    "    for k in range(m):   #do normalisation by looping through raw_data and subtracting the mean of each feature from each data point\n",
    "        for h in range(2352):\n",
    "            data[k][h]=data[k][h]-averages[h][0]\n",
    "    return data\n",
    "    #return (data - data.min())/ (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "    transposed=data.transpose() #to calculate covariance matrix, find transpose\n",
    "    intermediate=np.dot(transposed,data)\n",
    "    cov_matrix=(1/(2000-1))*intermediate #covariance matrix\n",
    "    # cov_matrix = np.cov(data.T) #covariance matrix\n",
    "    eigenvalues,eigenvectors = np.linalg.eig(cov_matrix) #eigenvalues and eigenvectors for covariance matrix\n",
    "    variation_matrix = []\n",
    "\n",
    "    for val in eigenvalues:\n",
    "        variation_matrix.append(val / np.sum(eigenvalues) * 100) #get the percentage of variation for each eigenvalue\n",
    "    \n",
    "    cumulative_variation = np.cumsum(variation_matrix) #cumulative variation -> get components that contribute most to the data\n",
    "    # print(cumulative_variation) #-> to see how many components are needed to explain the data (750 components are enough)\n",
    "\n",
    "\n",
    "    projected_data = np.dot(data,eigenvectors[:,:n]) #projected data\n",
    "\n",
    "    # U, Sigma, Vh = np.linalg.svd(data, full_matrices=False, compute_uv=True)\n",
    "    # projected_data = np.dot(U, np.diag(Sigma))\n",
    "    return projected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"inputs.txt\")\n",
    "labels = np.loadtxt(\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape #m is the number of data-points /samples, n is the number of features\n",
    "n = 750 #number of components to be used -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 750)\n"
     ]
    }
   ],
   "source": [
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "data = data[randomize]\n",
    "labels = labels[randomize]\n",
    "data=normalize(data,m)\n",
    "data = pca(data, n)\n",
    "\n",
    "#This is to avoid dimension issues with np.dot \n",
    "#Also...I took small samples of the date to test the ann\n",
    "training_data = data[:1200].T\n",
    "Y_training = labels[:1200]\n",
    "X_training = training_data[0:n]\n",
    "\n",
    "validation_data = data[1200:1600].T #note, includes start index, excludes end index\n",
    "Y_validation = labels[1200:1600]\n",
    "X_validation = validation_data[0:n]\n",
    "\n",
    "testing_data = data[1600:].T\n",
    "Y_testing = labels[1600:]\n",
    "X_testing = testing_data[0:n]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(): #generate a random set of weights and biases for the neural network between -1 and 1\n",
    "    w1= np.random.rand(5, n) - 1 # n is the number of features\n",
    "    b1 = np.random.rand(5, 1) - 1\n",
    "    w2 = np.random.rand(5, 5) - 1\n",
    "    b2 = np.random.rand(5, 1) - 1\n",
    "    w3 = np.random.rand(10, 5) - 1\n",
    "    b3 = np.random.rand(10, 1) - 1\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, b1, w2, b2, w3, b3): #forward propagation\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return A1, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels): #encode labels as one-hot vectors\n",
    "    labels = labels.astype(int)\n",
    "    encoded_labels = np.zeros((labels.size, 10))\n",
    "    for i in range(labels.size):\n",
    "        encoded_labels[i][labels[i]] = 1\n",
    "    return encoded_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[6. 7. 5. ... 1. 3. 6.]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encode(Y_training))\n",
    "print(Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(w1, w2, w3): #regularization (lambda = 0.99)\n",
    "    w1 = w1 * 0.99\n",
    "    w2 = w2 * 0.99\n",
    "    w3 = w3 * 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#w1, b1, w2, b2, w3, b3: weights and biases\n",
    "#X: training data\n",
    "#Y: training labels\n",
    "#A1, A2, A3: activation functions\n",
    "\n",
    "#Returns:\n",
    "#dw1, db1, dw2, db2, dw3, db3: deltas for weights and biases\n",
    "def backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3): #backpropagation\n",
    "    Y = one_hot_encode(Y)\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T)\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(w3.T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid_prime(A2)\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(w2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid_prime(A1)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#dW1, db1, dW2, db2, dW3, db3: deltas for weights and biases\n",
    "\n",
    "#Returns:   \n",
    "#w1, b1, w2, b2, w3, b3: updated weights and biases\n",
    "def update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate): #update parameters\n",
    "    w1 = w1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    w3 = w3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    regularize(w1, w2, w3) #regularize weights\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A3): \n",
    "    return np.argmax(A3,0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    accuracy = np.sum(predictions == Y) / Y.size\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, epochs, learning_rate): #gradient descent -> learn weights and biases\n",
    "    w1, b1, w2, b2, w3, b3 = rand_params()\n",
    "    for i in range(epochs):\n",
    "        A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3)\n",
    "        w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  2000 | Training Accuracy:  94.58333333333333 % | Learning Rate:  0.01\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "alpha = 0.01\n",
    "#learnt set of weights and biases (this is basically what we are submitting)\n",
    "w1, b1, w2, b2, w3, b3 = gradient_descent(X_training, Y_training, epochs, alpha)\n",
    "\n",
    "training_accuracy = get_accuracy(get_predictions(forward_prop(X_training, w1, b1, w2, b2, w3, b3)[2]), Y_training)\n",
    "print(\"Epochs: \",epochs,\"|\",\"Training Accuracy: \", training_accuracy,\"%\", \"|\", \"Learning Rate: \", alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  75.25 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: \", get_accuracy(get_predictions(forward_prop(X_validation, w1, b1, w2, b2, w3, b3)[2]), Y_validation), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  64.5 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: \", get_accuracy(get_predictions(forward_prop(X_testing, w1, b1, w2, b2, w3, b3)[2]), Y_testing), \"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fe92ade69d83066448b50cb9da733def79cf0a2a11147297fe11eaed3690119e"
  },
  "kernelspec": {
   "display_name": "Python 3.10.3 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
