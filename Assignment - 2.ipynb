{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition as skd \n",
    "#have a look at the documentation for skd here \n",
    "#\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data): #normalize data to be between 0 and 1\n",
    "    return (data - np.mean(data))/np.std(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n_components=2): #perform PCA on data to reduce dimensionality (change the n_components to see how it affects the results)\n",
    "    pca = skd.PCA(n_components=n_components)\n",
    "    pca.fit(data)\n",
    "    return pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = np.loadtxt(\"inputs.txt\")\n",
    "raw_data_labels = np.loadtxt(\"labels.txt\")\n",
    "#print(raw_data.size)\n",
    "\n",
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(raw_data_labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "raw_data = raw_data[randomize]\n",
    "raw_data_labels = raw_data_labels[randomize]\n",
    "\n",
    "# print(raw_data)\n",
    "# print(raw_data_labels)\n",
    "\n",
    "#split into training, validation, testing\n",
    "training_data = raw_data[:1200] \n",
    "training_data_labels = raw_data_labels[:1200] \n",
    "\n",
    "validation_data = raw_data[1200:1600] #note, includes start index, excludes end index\n",
    "validation_data_labels = raw_data_labels[1200:1600]\n",
    "\n",
    "testing_data = raw_data[1600:2000]\n",
    "testing_data_labels = raw_data_labels[1600:2000]\n",
    "\n",
    "print(validation_data_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(): #generate a random set of weights and biases for the neural network between -1 and 1\n",
    "    w1= np.random.rand(1000, 2352) - 1\n",
    "    b1 = np.random.rand(1000, 1) - 1\n",
    "    w2 = np.random.rand(1000, 1000) - 1\n",
    "    b2 = np.random.rand(1000, 1) - 1\n",
    "    w3 = np.random.rand(10, 1000) - 1\n",
    "    b3 = np.random.rand(10, 1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, b1, w2, b2, w3, b3): #forward propagation\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return A1, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels): #encode labels as one-hot vectors\n",
    "    encoded_labels = np.zeros((labels.size, 9))\n",
    "    for i in range(labels.size):\n",
    "        encoded_labels[i][labels[i]] = 1\n",
    "    return encoded_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parameter:\n",
    "# (w1, b1, w2, b2, w3, b3) = weight and biases for each layer\n",
    "# X = input data\n",
    "# Y = labels\n",
    "# lr = learning rate for gradient descent (aka alpha)\n",
    "\n",
    "# Refer to \"https://towardsdatascience.com/math-neural-network-from-scratch-in-python-d6da9f29ce65\",\n",
    "# \"https://drive.google.com/file/d/1NVle5nlr0m2OHDiSTReSv3TCzMJGu27s/view?usp=sharing\"\n",
    "#\"https://drive.google.com/file/d/1NVle5nlr0m2OHDiSTReSv3TCzMJGu27s/view?usp=sharing\"\n",
    "# It basically calculates the error of activations layers and then backpropagates the error to the previous layer\n",
    "def update_params(w1, b1, w2, b2, w3, b3, A1, A2, A3, X, Y, lr): \n",
    "    dZ3 = A3 - Y #delta for the output layer\n",
    "    dW3 = np.dot(dZ3, A2.T) #delta for the weights of the output layer\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True) #delta for the biases of the output layer\n",
    "    dZ2 = np.dot(w3.T, dZ3) * sigmoid_prime(A2) #delta for the hidden layer\n",
    "    dW2 = np.dot(dZ2, A1.T) #delta for the weights of the hidden layer\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)#delta for the biases of the hidden layer\n",
    "    dZ1 = np.dot(w2.T, dZ2) * sigmoid_prime(A1) #delta for the hidden layer\n",
    "    dW1 = np.dot(dZ1, X.T) #delta for the weights of the hidden layer\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True) #delta for the biases of the hidden layer\n",
    "    w1, w2, w3 = regularize(w1,w2,w3)\n",
    "    w1 = w1 - lr * dW1\n",
    "    b1 = b1 - lr * db1\n",
    "    w2 = w2 - lr * dW2\n",
    "    b2 = b2 - lr * db2\n",
    "    w3 = w3 - lr * dW3\n",
    "    b3 = b3 - lr * db3\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(w1, w2, w3): #regularization (lambda = 0.99)\n",
    "    w1 = w1 * 0.99\n",
    "    w2 = w2 * 0.99\n",
    "    w3 = w3 * 0.99\n",
    "\n",
    "    return w1, w2, w3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(): #perform gradient descent / learn neural network parameters\n",
    "    w1, b1, w2, b2, w3, b3 = rand_params()\n",
    "    for i in range(10000):\n",
    "        A1, A2, A3 = forward_prop(training_data, w1, b1, w2, b2, w3, b3)\n",
    "        w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, A1, A2, A3, training_data, training_data_labels, 0.01)\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#learnt set of parameters\n",
    "w1, b1, w2, b2, w3, b3 = gradient_descent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prediction(X, w1, b1, w2, b2, w3, b3): #make a prediction\n",
    "    A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "    return np.argmax(A3, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_prediction(X, Y, w1, b1, w2, b2, w3, b3): #test the prediction -> calculate accuracy\n",
    "    prediction = get_prediction(X, w1, b1, w2, b2, w3, b3)\n",
    "    return np.mean(prediction == Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = get_prediction(validation_data, w1, b1, w2, b2, w3, b3)\n",
    "accuracy = test_prediction(validation_data, validation_data_labels, w1, b1, w2, b2, w3, b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(X, w1, b1, w2, b2, w3, b3): #classify data using learnt parameters -> for stdin input\n",
    "    A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "    return np.argmax(A3, axis=0)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce3b179e982818025b1b8f06d5ea688e0dcbdafdcad235aa6481c10d2e91eb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
