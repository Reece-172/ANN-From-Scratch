{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1476,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1477,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data,m):#normalize the data\n",
    "    avgs=data.mean(axis=0) #calculates the means of each feature (each column)\n",
    "    averages=avgs.reshape((2352,1))\n",
    "    for k in range(m):   #do normalisation by looping through raw_data and subtracting the mean of each feature from each data point\n",
    "        for h in range(2352):\n",
    "            data[k][h]=data[k][h]-averages[h][0]\n",
    "    return data\n",
    "    #return (data - data.min())/ (data.max() - data.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1478,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "    transposed=data.transpose() #to calculate covariance matrix, find transpose\n",
    "    intermediate=np.dot(transposed,data)\n",
    "    cov_matrix=(1/(2000-1))*intermediate #covariance matrix\n",
    "    # cov_matrix = np.cov(data.T) #covariance matrix\n",
    "    eigenvalues,eigenvectors = np.linalg.eig(cov_matrix) #eigenvalues and eigenvectors for covariance matrix\n",
    "    variation_matrix = []\n",
    "\n",
    "    for i in eigenvalues:\n",
    "        variation_matrix.append(i/np.sum(eigenvalues)*100) #get the percentage of variation for each eigenvalue\n",
    "    \n",
    "    cumulative_variation = np.cumsum(variation_matrix) #cumulative variation -> get components that contribute most to the data\n",
    "    # print(cumulative_variation) #-> to see how many components are needed to explain the data (10 components are enough)\n",
    "\n",
    "\n",
    "    projected_data = np.dot(data,eigenvectors[:,:n]) #projected data\n",
    "\n",
    "    # U, Sigma, Vh = np.linalg.svd(data, full_matrices=False, compute_uv=True)\n",
    "    # projected_data = np.dot(U, np.diag(Sigma))\n",
    "    return projected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1479,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"inputs.txt\")\n",
    "labels = np.loadtxt(\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1480,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape #m is the number of data-points /samples, n is the number of features\n",
    "n = 750 #number of components to be used -> PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000, 750)\n"
     ]
    }
   ],
   "source": [
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "data = data[randomize]\n",
    "labels = labels[randomize]\n",
    "data=normalize(data,m)\n",
    "data = pca(data, n)\n",
    "\n",
    "#This is to avoid dimension issues with np.dot \n",
    "#Also...I took small samples of the date to test the ann\n",
    "training_data = data[:1200].T\n",
    "Y_training = labels[:1200]\n",
    "X_training = training_data[0:n]\n",
    "\n",
    "validation_data = data[1200:1600].T #note, includes start index, excludes end index\n",
    "Y_validation = labels[1200:1600]\n",
    "X_validation = validation_data[0:n]\n",
    "\n",
    "testing_data = data[1600:].T\n",
    "Y_testing = labels[1600:]\n",
    "X_testing = testing_data[0:n]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1482,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1483,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1484,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1485,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(): #generate a random set of weights and biases for the neural network between -1 and 1\n",
    "    w1= np.random.rand(5, n) - 1 # n is the number of features\n",
    "    b1 = np.random.rand(5, 1) - 1\n",
    "    w2 = np.random.rand(5, 5) - 1\n",
    "    b2 = np.random.rand(5, 1) - 1\n",
    "    w3 = np.random.rand(10, 5) - 1\n",
    "    b3 = np.random.rand(10, 1) - 1\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1486,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, b1, w2, b2, w3, b3): #forward propagation\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return A1, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1487,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels): #encode labels as one-hot vectors\n",
    "    labels = labels.astype(int)\n",
    "    encoded_labels = np.zeros((labels.size, 10))\n",
    "    for i in range(labels.size):\n",
    "        encoded_labels[i][labels[i]] = 1\n",
    "    return encoded_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 1.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 1. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "[6. 8. 6. ... 2. 6. 1.]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encode(Y_training))\n",
    "print(Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1489,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(w1, w2, w3): #regularization (lambda = 0.99)\n",
    "    w1 = w1 * 0.99\n",
    "    w2 = w2 * 0.99\n",
    "    w3 = w3 * 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1490,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#w1, b1, w2, b2, w3, b3: weights and biases\n",
    "#X: training data\n",
    "#Y: training labels\n",
    "#A1, A2, A3: activation functions\n",
    "\n",
    "#Returns:\n",
    "#dw1, db1, dw2, db2, dw3, db3: deltas for weights and biases\n",
    "def backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3): #backpropagation\n",
    "    Y = one_hot_encode(Y)\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T)\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(w3.T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid_prime(A2)\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(w2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid_prime(A1)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#dW1, db1, dW2, db2, dW3, db3: deltas for weights and biases\n",
    "\n",
    "#Returns:   \n",
    "#w1, b1, w2, b2, w3, b3: updated weights and biases\n",
    "def update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate): #update parameters\n",
    "    w1 = w1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    w3 = w3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    regularize(w1, w2, w3) #regularize weights\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1492,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A3): \n",
    "    return np.argmax(A3,0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    accuracy = np.sum(predictions == Y) / Y.size\n",
    "    return accuracy * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1493,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, epochs, learning_rate): #gradient descent -> learn weights and biases\n",
    "    w1, b1, w2, b2, w3, b3 = rand_params()\n",
    "    for i in range(epochs):\n",
    "        A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3)\n",
    "        w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1494,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  2000 | Training Accuracy:  96.75 % | Learning Rate:  0.01\n"
     ]
    }
   ],
   "source": [
    "epochs = 2000\n",
    "alpha = 0.01\n",
    "#learnt set of weights and biases (this is basically what we are submitting)\n",
    "w1, b1, w2, b2, w3, b3 = gradient_descent(X_training, Y_training, epochs, alpha)\n",
    "\n",
    "training_accuracy = get_accuracy(get_predictions(forward_prop(X_training, w1, b1, w2, b2, w3, b3)[2]), Y_training)\n",
    "print(\"Epochs: \",epochs,\"|\",\"Training Accuracy: \", training_accuracy,\"%\", \"|\", \"Learning Rate: \", alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy:  59.5 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Validation Accuracy: \", get_accuracy(get_predictions(forward_prop(X_validation, w1, b1, w2, b2, w3, b3)[2]), Y_validation), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1496,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy:  64.5 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Testing Accuracy: \", get_accuracy(get_predictions(forward_prop(X_testing, w1, b1, w2, b2, w3, b3)[2]), Y_testing), \"%\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6e9183538a4bd9d3d5f078f6e27e4411a652c1fd77b326d0148744f7dd1f4df6"
  },
  "kernelspec": {
   "display_name": "Python 3.7.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
