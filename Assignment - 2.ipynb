{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 321,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn.decomposition as skd \n",
    "#have a look at the documentation for skd here \n",
    "#\"https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(data):\n",
    "    return (data - data.mean())/data.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "#     normalize(data)    \n",
    "#     pca = skd.PCA(n_components = n)\n",
    "#     pca.fit(data)\n",
    "#     return pca.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "#     transposed = data.transpose() #to calculate covariance matrix, find transpose\n",
    "#     intermediate = np.dot(transposed,data)\n",
    "#     cov_matrix = (1/(2000-1)) * intermediate #covariance matrix\n",
    "\n",
    "#     #print(cov_matrix.shape)\n",
    "\n",
    "#     eigenvalues,eigenvectors = np.linalg.eig(cov_matrix) #eigenvalues and eigenvectors for covariance matrix\n",
    "#     #print(len(eigenvalues))\n",
    "#     # print((eigenvectors.shape))\n",
    "\n",
    "#     variation_matrix=np.zeros((2352,1)) # corresponds to each eigenvalue to see which contributes most to the data\n",
    "#     summation = np.sum(eigenvalues)\n",
    "#     for i in range(2352):\n",
    "#         variation_matrix[i][0]=eigenvalues[i]/summation\n",
    "\n",
    "#     reduced_eigenvectors = eigenvectors[:,:n] #2352x21\n",
    "#     print(reduced_eigenvectors.shape)\n",
    "#     return np.dot(data,reduced_eigenvectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca(data, n): #perform PCA on data to reduce dimensionality (change the n_components / features to see how it affects the results)\n",
    "    cov_matrix = np.cov(data.T) #covariance matrix\n",
    "    eigenvalues,eigenvectors = np.linalg.eig(cov_matrix) #eigenvalues and eigenvectors for covariance matrix\n",
    "    variation_matrix = []\n",
    "\n",
    "    for i in eigenvalues:\n",
    "        variation_matrix.append(i/np.sum(eigenvalues)*100) #get the percentage of variation for each eigenvalue\n",
    "    \n",
    "    cumulative_variation = np.cumsum(variation_matrix) #cumulative variation -> get components that contribute most to the data\n",
    "    print(cumulative_variation) #-> we can use 10 componets to explain most of the features\n",
    "    #We only use the above code if we want to see the percentage of variation for each eigenvalue\n",
    "\n",
    "    projected_data = np.dot(data,eigenvectors[:,:n]) #projected data\n",
    "    return projected_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt(\"inputs.txt\")\n",
    "labels = np.loadtxt(\"labels.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "m, n = data.shape #m is the number of data-points /samples, n is the number of features\n",
    "n = 10 #testing with 5 features out of 2352"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 21.52663766+0.j  35.52385919+0.j  48.52958238+0.j  52.28408258+0.j\n",
      "  56.64006322+0.j  62.69159041+0.j  70.3856342 +0.j  78.93483253+0.j\n",
      "  88.75218839+0.j 100.        +0.j]\n",
      "(2000, 10)\n"
     ]
    }
   ],
   "source": [
    "#code to shuffle 2 arrays, and keep corresponding elements\n",
    "randomize = np.arange(len(labels)) \n",
    "np.random.shuffle(randomize) #creates a randomized sequence to be used as an index for the two arrays to shuffle them (https://www.delftstack.com/howto/numpy/python-numpy-shuffle-two-arrays/)\n",
    "\n",
    "data = data[randomize]\n",
    "labels = labels[randomize]\n",
    "\n",
    "data = pca(data, n)\n",
    "\n",
    "#This is to avoid dimension issues with np.dot \n",
    "#Also...I took small samples of the date to test the ann\n",
    "training_data = data[:10].T\n",
    "Y_training = labels[:10]\n",
    "X_training = training_data[0:n]\n",
    "\n",
    "validation_data = data[10:16].T #note, includes start index, excludes end index\n",
    "Y_validation = labels[10:16]\n",
    "X_validation = training_data[0:n]\n",
    "\n",
    "testing_data = data[16:24].T\n",
    "Y_testing = labels[16:24]\n",
    "X_testing = training_data[0:n]\n",
    "\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    return 1/(1 + np.exp(-Z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_prime(Z):\n",
    "    return Z * (1 - Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(Z):\n",
    "    return np.exp(Z)/np.sum(np.exp(Z), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rand_params(): #generate a random set of weights and biases for the neural network between -1 and 1\n",
    "    w1= np.random.rand(5, n) - 1 # n is the number of features\n",
    "    b1 = np.random.rand(5, 1) - 1\n",
    "    w2 = np.random.rand(5, 5) - 1\n",
    "    b2 = np.random.rand(5, 1) - 1\n",
    "    w3 = np.random.rand(10, 5) - 1\n",
    "    b3 = np.random.rand(10, 1) - 1\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_prop(X, w1, b1, w2, b2, w3, b3): #forward propagation\n",
    "    Z1 = np.dot(w1, X) + b1\n",
    "    A1 = sigmoid(Z1)\n",
    "    Z2 = np.dot(w2, A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "    Z3 = np.dot(w3, A2) + b3\n",
    "    A3 = softmax(Z3)\n",
    "    return A1, A2, A3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(labels): #encode labels as one-hot vectors\n",
    "    labels = labels.astype(int)\n",
    "    encoded_labels = np.zeros((labels.size, 10))\n",
    "    for i in range(labels.size):\n",
    "        encoded_labels[i][labels[i]] = 1\n",
    "    return encoded_labels.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 1. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[2. 4. 4. 6. 0. 2. 1. 7. 2. 7.]\n"
     ]
    }
   ],
   "source": [
    "print(one_hot_encode(Y_training))\n",
    "print(Y_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularize(w1, w2, w3): #regularization (lambda = 0.99)\n",
    "    w1 = w1 * 0.99\n",
    "    w2 = w2 * 0.99\n",
    "    w3 = w3 * 0.99"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#w1, b1, w2, b2, w3, b3: weights and biases\n",
    "#X: training data\n",
    "#Y: training labels\n",
    "#A1, A2, A3: activation functions\n",
    "\n",
    "#Returns:\n",
    "#dw1, db1, dw2, db2, dw3, db3: deltas for weights and biases\n",
    "def backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3): #backpropagation\n",
    "    Y = one_hot_encode(Y)\n",
    "    dZ3 = A3 - Y\n",
    "    dW3 = np.dot(dZ3, A2.T)\n",
    "    db3 = np.sum(dZ3, axis=1, keepdims=True)\n",
    "    dA2 = np.dot(w3.T, dZ3)\n",
    "    dZ2 = dA2 * sigmoid_prime(A2)\n",
    "    dW2 = np.dot(dZ2, A1.T)\n",
    "    db2 = np.sum(dZ2, axis=1, keepdims=True)\n",
    "    dA1 = np.dot(w2.T, dZ2)\n",
    "    dZ1 = dA1 * sigmoid_prime(A1)\n",
    "    dW1 = np.dot(dZ1, X.T)\n",
    "    db1 = np.sum(dZ1, axis=1, keepdims=True)\n",
    "    return dW1, db1, dW2, db2, dW3, db3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters:\n",
    "#dW1, db1, dW2, db2, dW3, db3: deltas for weights and biases\n",
    "\n",
    "#Returns:   \n",
    "#w1, b1, w2, b2, w3, b3: updated weights and biases\n",
    "def update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate): #update parameters\n",
    "    w1 = w1 - learning_rate * dW1\n",
    "    b1 = b1 - learning_rate * db1\n",
    "    w2 = w2 - learning_rate * dW2\n",
    "    b2 = b2 - learning_rate * db2\n",
    "    w3 = w3 - learning_rate * dW3\n",
    "    b3 = b3 - learning_rate * db3\n",
    "    regularize(w1, w2, w3) #regularize weights\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions(A3): \n",
    "    return np.argmax(A3,0)\n",
    "\n",
    "def get_accuracy(predictions, Y):\n",
    "    return np.sum(predictions == Y) / Y.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(X, Y, epochs, learning_rate): #gradient descent -> learn weights and biases\n",
    "    w1, b1, w2, b2, w3, b3 = rand_params()\n",
    "    for i in range(epochs):\n",
    "        A1, A2, A3 = forward_prop(X, w1, b1, w2, b2, w3, b3)\n",
    "        dW1, db1, dW2, db2, dW3, db3 = backprop(X, Y, A1, A2, A3, w1, b1, w2, b2, w3, b3)\n",
    "        w1, b1, w2, b2, w3, b3 = update_params(w1, b1, w2, b2, w3, b3, dW1, db1, dW2, db2, dW3, db3, learning_rate)\n",
    "    \n",
    "    print(\"Epochs: \",epochs,\"|\",\"Training Accuracy: \", get_accuracy(get_predictions(A3), Y))\n",
    "    print(\"Predicted: \",get_predictions(A3),\"\\n\", \"Actual: \",Y)\n",
    "    return w1, b1, w2, b2, w3, b3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs:  2000 | Training Accuracy:  1.0\n",
      "Predicted:  [8 8 6 8 8 1 9 9 4 4] \n",
      " Actual:  [8. 8. 6. 8. 8. 1. 9. 9. 4. 4.]\n"
     ]
    }
   ],
   "source": [
    "#learnt set of weights and biases (this is basically what we are submitting)\n",
    "w1, b1, w2, b2, w3, b3 = gradient_descent(X_training, Y_training, 2000, 0.01)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "fce3b179e982818025b1b8f06d5ea688e0dcbdafdcad235aa6481c10d2e91eb0"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
